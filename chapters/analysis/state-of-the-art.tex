\section{State-of-the-art}

As has been already stated, the DSW application is being developed in cooperation of ELIXIR CZ and ELIXIR NL research groups.
The development is currently split into multiple independent parts.

The whole system is available as an open-source\footnote{Application repositories are available at GitHub under the organization page at https://github.com/ds-wizard.}.
That said, the source code, documentation, and the development process are publicly accessible and open to contribution.

The most significant part of the project is the server application called DSW-Server (in \gls{dsw} terminology called \textit{portal}).
This application implements all the functionality and business logic of the system.
Using standard terminology, we can refer to it as a backend.

Since the portal application does not provide any \gls{ui} (it only exposes \glsentryshort{api} using \glsentryshort{rest} standard), there is an complementary web application called DSW-Client (or, in \gls{dsw} terminology, called just \textit{client} for short) which does just that.
Using standard terminology, such application is refered to as a frontend.

\todo{Make sure the server part description and development state are both correct}

\subsection{DSW-Server}

The backend part of the application is written using the Haskell programming language.
The primary goal of this portal is to create data management plans. \reword{Most of the sentences starts with the word "the", reword it so it does not feel as machine generated.}
At least one knowledge model must be added to the system before data plans can be generated.
At the time of writing, \gls{dsw} is shipped with a prebuild knowledge model called \textit{core}.

Knowledge models are represented using \gls{json} and stored in a file on the server file system.
Data stewards then use these models to create questionaries.
The role of researchers is to fill in the questionaries with meta information about their research.

Filled-in questionaries are used by data stewards to create management plans.
This flow mostly covers system features.

We, however, have silent assumptions in the flow described above.
The portal also has to support the following features in addition to the mentioned business logic:

\begin{itemize}
    \item User management (roles, authorization and authentication),
    \item Package administration (cration, editing and versioning),
    \item Data persistency (database connection, file system integration)
    \item \glsentryshort{rest} \glsentryshort{api}.
\end{itemize}

All of the mentioned functions are split into independent Haskell modules.
The \glsentryshort{rest} \glsentryshort{api} layer is built using Scotty framework\footnote{Scotty is an open-source web framework written in Haskell, inspired by Ruby's Sinatra. It is a lightweight alternative to frameworks like Yesod or Spock. The source code is available on GitHub: https://github.com/scotty-web/scotty.}.

\subsection{DSW-Client}

DSW-Client is a frontend application written in Elm programming language\footnote{Elm is a type-safe programming language used for web applications frontend. The source code written in Elm is transpiled into JavaScript and interpreted using a web browser. Elm website: https://elm-lang.org.}.
It provides a web-based user interface for the server application.

The implementation was done by Ing. Jan Slifka as a part of his master's thesis in 2018.
\todo{Validate the date of frontend application}

The application is built using Elm Architecture.
The logic of such application is split into three separated parts:

\begin{description}
    \item[Model] -- represents applicaton state,
    \item[Update] -- allows to update the state,
    \item[View] -- interprets state using HTML.
\end{description}

This architecture has a similar approach to state modification as a popular state container called Redux.
Redux is also used for frontend applications, most commonly with React view library.

In oppose to React and Redux, Elm language was built with an architecture pattern in mind from the beginning.
According to the official website\cite{elm-speed}, Elm is up to two times faster than same application written in React.

The application is fully rendered locally in the user's web browser.
That makes the server and client applications fully independent of each other.
Development of both parts is, therefore, break up into two separate repositories.

\subsection{System architecture}

In the first part of this section, I will describe the architecture of server-side of the project which is writtent in Haskell.
The second part is dedicated to the front end application.

\subsubsection*{Server-side application}

As mentioned earlier the server part of the application is written using Haskell programming language, specifically using Scotty web framework.
The application expose public \gls{rest} \glsentryshort{api} over \glsentryshort{http} protocol.

To achieve modularity and loose coupling, the application is split into multiple modules.
Those modules are grouped into logical partitions based on their purpose.
The most significant parts are:

\begin{itemize}
    \item Handler layer,
    \item Service \gls{dto} layers,
    \item Model and \gls{dao} layers,
\end{itemize}

\paragraph*{Handler layer}\label{sec:handler} is responsible for processing \gls{api} requests.
It directly interacts with Scotty framework, transforms incomming data into strongly typed objects and orchestrates other layers to evaluate response.

Handler groups together multiple endopoints.
Those endpoints are uniquely registred to specific \gls{url} and \gls{http} method.
In terms of server application, we refer to such pair as \textit{route}.

\hscode{code:router-handler}{Routes definition and route endpoint (simplified)}{router-handler.hs}

Once the correct handler for requested route is selected, it starts processing data from request.
Such data might be \gls{url} (or query) parameters and request body.
Those data are transformed into language primitives (such as \texttt{Int} or \texttt{String}) or complex objects called \gls{dto} (explained later).
In case of invalid or malformed data, the request is aborted immediately with appropriate error information in response.

In addition to that, some routes may also require special permissions in order to be executed.
Those routes needs requests to be authenticated using \gls{jwt} technology and user tied with given token must be granted such permissions.

If the request is validated successfully, required data are passed into service layer where the actual business logic happens.

\paragraph*{Service layer} is responsible for application logic.
This means, that its public interface is exposed to other layers using \gls{dto}.
Private functions and internal dependencies (such as data persistence) are thus \textit{implementation detail} of the layer itself and does not affect its interface.

In \gls{dsw}, service layer takes care of basic \gls{crud} operations over persisted entities.
This includes basic objects, for example \texttt{User}, \texttt{KnowledgeModel} and \texttt{Questionnaire}.

In addition to data transformation, this layer also manages application configuration, knowledge model migrations, \gls{dto} mapping and maintains data consistency.
To keep the public interface simple, services are usually composed using other services or \gls{dao}.

Services makes operations based on given input data (objects identifiers, \gls{dto}, \dots).
Operations itself then converts \gls{dto} objects into internal representation (persisted object) and do computations on it.
As a result of computation, \gls{dto} object is returned from service.

\paragraph*{\gls{dto}} While describing Service layer earlier in this chapter, there were multiple references to objects called \gls{dto}.
These objects' goal is to create framework-independent interface between layers.
\gls{dto} are plain objects\footnote{By plain objects we usually mean structures, which are not restrcited by any framework and are built only constructs available in language itself or by other plain objects. Same pattern is available in other languages. In Java programming language, same concept is refered to as \gls{pojo}\cite{wiki-pojo}.} representing standard model objects, usually in many ways.

Model objects may be complex structures containing significant amount of data.
Such complexity may or may not be appropriate for some service operations.

While presenting list of data (populated by \gls{dto}) to the user, it is not necessary to fetch all nested objects recursively.
Instead, simplified objects may be used and additional data may be requested on demand.
On the other hand, when user want to see detail of some list item, \gls{dto} object should contain enough data to fulfill user's expectaion.
For such usecase, there would be two different objects and service would choose the one which best fits user expectation.

\hscode{code:dto}{DTO definition and transformation from Model object}{dto.hs}

In \gls{dsw} this approach is used to display either list or detail of questionnaires, knowledge models or knowledge model customizations.
Conversion between model and \gls{dto} objects are done using specific services.

\paragraph*{Authentication} In section \ref{sec:handler} about \gls{http} requests handling, I briefly discussed endpoint authentication using \gls{jwt}.
In this section the reader will be acquainted with how these tokens work.

Even though some endpoints (such as login or registration) do not require user to be authenticated, vast majority of the application is based on managing private data.
Therefore, the user needs to be authenticated using username and password before he or she can access these data.
Similary to data persistence, also authentication may be done in many ways.
There is basic authorization\footnote{Base64 is an HTTP authentication used to directly encode user's credentials into reuqest header. Such approach is considered as insecure as the credentials are easily captured by unauthorized person\cite{qnimate-base64}.}, API keys, \gls{jwt} and many more.

The \gls{dsw} application uses \gls{jwt} tokens, which stands out by having ability to be verified for issuer authenticity.
Thanks to this, the issuer (in this case server application) can embed custom payload into token and be sure those data will not be modified by unauthorized person.

To become authorized, user first have to log in to application using username and password.
If those credentials are correct, server will issue access token and send it back to the user.
For all request requiring authorization, the user has to send issued token in request header.
Server will read the payload, verify token integrity and check his permissions.
If the token is valid, appropriate handler is called.

All \gls{jwt} tokens have same structure (pictured in figure \ref{fig:jwt-token}).
The token consists from three parts: \texttt{Header}, \texttt{Payload} and \texttt{Signature}.

\image[0.8\textwidth]{fig:jwt-token}{JWT token structure example}{jwt-example.pdf}

Header contains information about token type and hashing algorithm used to generate signature.
Payload is arbitrary \gls{json} object containing publicly visible data (such as user identifier).
Token signature is created by hashing header and payload (both encoded using Base64 algorithm) by hashing algorithm specified in header and also encoded using Base64.

These three parts are combined using period (.) character and returned as \texttt{String}.

\paragraph*{Authorization}

In modern application, authenticating users is not enough.
We might want a system to support to support wide range users hierarchy, capabilities and responsibilities.
In \gls{dsw} this is solved using combinations of roles and permissions.

Each user has one of the following roles:

\begin{itemize}
    \item Administrator,
    \item Data Steward,
    \item Researcher
\end{itemize}

Once user is created, he has assinged role and default set of permissions.
Having two levels of authorization is an important system feature as user's account might gain or lose priviledges over its lifetime.

Permissions are resource specific and for each resource we might distinguish between multiple permissions (for example read and write permissions).
Default permissons are system-wide configurable using configuration file (which is loaded at startup time).

In default environment, Administrator has all possible permissions including user, organization and content management.
The role in heirarchy just under the Administrator -- Data Steward -- has similar permissions but is unable to manage organizations and other users.
Research is able to see knowledge models and public questionnaires, but is able to only modify content he created.

As mentioned earlier in section \ref{sec:handler}, permissions are checked in handler before any business logic happens.
Each handler has defined which permission is required (for example \texttt{Read questionnaire}).
Once the user makes request, handler checks authentication state and search user's permissions (demonstrated in code example \ref{code:router-handler}).
If user has granted required permissions, request processing continues.
Otherwise the request is aborted immediately.

\paragraph*{\gls{dao}}\label{sec:dao} is a way to access persisted data using simplified interface.
In general, application data may be persisted in many ways.
The most common approach to this is using database. \todo{Add cite here}
But even database persitence may be implemented in different ways.

On the marked, there are several options available.
There are various kinds of databases: Graph databases, Relational databases, Document databases and many others.
Moreover, all of those offered as free to use as well as business solutions.

In \gls{dsw}, the Mongo DB was chosen as a dabatase for persisting data.
Mongo DB is an NoSQL document databse (explained later), which allows to store data using nested structures.

The goal of \gls{dao} is to encapsulate such technical detail from other application layers.
The underlying database may change over time, but as long as data model stays the same, the only affected layer will be \gls{dao}.

In Haskell, such layer is implemented using independent modules where each module manages one resource (Mongo DB collection).
The module public interface is implemented using free functions.
Those functions offer high level API such as \texttt{findAll}, \texttt{findById} and similar for all \gls{crud} operations.

\hscode{code:dao}{DAO module example for Questionnaire entity}{dao.hs}

\paragraph*{Mongo DB}\label{sec:mongo-db} was chosen as a persistent layer in early development of \gls{dsw}.
As stated earlier in section \ref{sec:dao}, Mongo DB is an document database.

By document document dabase, we understand system which is able to store hierarchical tree structures (so called \textit{docuemnts}) composed using scalars (\texttt{String}, \texttt{Integer}, \dots), hashable maps, arrays or nested documents.
These documents are identified using internal unique identifiers and grouped into collections.

On collections, we can run standard \gls{crud} queries to manipulate with stored data.
Mongo DB provides \gls{api} for querying objects using notation based on \gls{json}.
This notation allows to query on nested objects, aggregations, relations or regular expressions.
Since \gls{json} notation may be unnecessarily verbose, documents are internaly stored in \gls{bson} representation.

\jscode{code:mongo-find-one}{Mongo DB query for finding questionnaire using its identifier}{mongodb-findOne.js}

In oppose to standard relational databases, collections and documents does not have an explicit schema.
Therefore there might be stored almost any kind of data in collection at one time.

Another notable difference is in data normalization.
Standard databases (based on \gls{sql}) use data decompozition and normalization to achieve better performance and great organisation of tables and columns.
In case of Mongo DB, related documents are usually tightly coupled together (using composition) and duplicated instead of relations using foreign keys.

As mentioned before, the data are internally stored in binary format which is not directly usable in Haskell.
Therefore the driver expose two special Typeclasses\footnote{More information about Haskell Typeclasses are available at https://www.haskell.org/tutorial/classes.html} for encoding and decoding binary formatted data.
Such typeclasses must be implemented by all types which will be stored in database.

\hscode{code:bson-mapping}{Example of Typeclasses used to transform Model object into binary representation}{haskell-bson-mapping.hs}

In addition to coding Typeclasses, the driver also expose type-safe interface to build \gls{crud} queries.
These queries are created using simple \gls{dsl} as shown in example \ref{code:dao}.

\subsection{Frontend application}

\image[0.5\textwidth]{fig:system-architecture}{System architecture}{analysis-system-architecture.pdf}

\subsection{Knowledge model migrations}\label{sec:km-migrations}

Initially, the \gls{dsw} portal was created as an user-friendly alternative to knowledge model management which was built using \gls{json} notation.
This notation was originally created and maintained by Rob Hooft\footnote{Rob Hooft is an manager of Netherland node of ELIXIR group (ELIXIR NL). Mr. Hoft created the initial knowledge model data source which is currently used in \gls{dsw} as an optional component.} and is still used as core knowledge model in \gls{dsw}.

As knowledge models may change over time, it is required to keep track of all possible versions created in past.
In terms of \gls{dsw}, such process of modifying knowledge model is called migration.
The migration process was designed and implemented by Ing. VojtÄ›ch Knaisl as a part of his master thesis\cite{mt-knaisl}.

Migration consists of two parts: modification and upgrade.
In the modification part, data steward may do \gls{crud} operations over all knowledge model nodes (this includes chapters, questions, answers, references and experts).
Once the modifications are done, data steward publish new version of knowledge model so the modifications are available to other stewards.

The second part, upgrade, is done on knowledge model customizations.
These customizations may be for example localizations, which needs to reflect changes in its parent knowledge model.
During upgrade, system asks data steward step by step for all knowledge model chagnes comming from parent knowledge model.
Since modifications on a single node may be done on both customizations and parent knowledge model, conflicts may occure.

In case of conflict, user is asked to solve it manually.
There are two options for user to solve conflict.
User may either accept incomming change (which will discard customization changes) or reject incomming change (which will in oppose prefer customization changes).
Changes which do not cause conflict are merged automatically without user interaction.

Once all conflicts are resolve, data steward is asked to complete migration by publishing new version of customization.

On top of knowledge models, reasearchers creates and fills questionnaires which are later used to create data management plans.
Questionnaires are currently tighly coupled to knowledge model they were initially created on.
This however means, that once new version of knowledge model is released, new questionnaire has to be created and filled from scratch.

As a result of this master's thesis, researchers will be able to migrate their questonnaires to newer version similary as data stewards are able to migrate knowledge models.

\subsection{Event-driven architecture}

In previous section \ref{sec:km-migrations}, I discussed the idea behind having multiple versions of the same data source at once.
In this section, I would like to briefly describe the technical details of that idea.

In system representation, knowledge models are nothing more than sequence of modifiation events.
Those events are strictly defined and must be always performed in order they were created.
Single event represents one specific modification.
This modification may be "Add chapter", "Add question" or "Remove answer".
Together, system supports exactly twenty events where each of these events contains additional meta data.
In Haskell, such structure is represented using multiple constructors of the same data type (example \ref{code:events}).

\hscode{code:events}{Event representation in \gls{dsw}}{events.hs}

This approach is generally known as \textit{event-driven architecture}\cite{mdm-event-architecture}.
One of the main benefits for \gls{dsw} is that by repeatedly applying same events again, we always get same result.

This works well with Mongo DB as persistence layer.
Instead of compiling (by applying events) knowledge models and storing result in databse multiple times (for each custoization, questionnaire, \dots), only events are stored.
When user requests compiled knowledge model, it is always compiled on demand presented using dto.
Thanks to that, user always has the latest possible version without problems with inconsistency which would be possible over time.

Another great feature of chain of events is that it can be easily manipulated.
For example, merging two customizations of knowledge model may be easily done by applying one set of events on the end of the other chain.
It also allows user to "cherry pick" events and apply only subset of incomming changes.
Those properties are greatly utilized in knowledge model migrations mentioned earlier.

\subsection{Deployment}

In previous sections I described the architecture of \gls{dsw} distributed system.
This section dedicated to production environment of this application.

Since both parts of the system are independent, deployment of the whole app is done in two steps.
In one step, the server side application is deployed.
As discussed earlier, Haskell application does not need application server (the requests are handled directly by the application) but depends on persistence layer.
To make the deployed application lose coupled, two Docker containers\footnote{What is docker} are used to decouple application itself from database.
To connect the containers, Docker compose is used to make private network between them.
From the user perspective, the application behaves as monolith which internally encapsulates its complexity.

In the second step, the client application is deployed in similar manner.
Since Elm application actually runs in user's internet browser, there is an additional container which handles \gls{http} requests and servers the application to the user.
This container may ber arbitrary \gls{http} server.
For purpose of \gls{dsw}, the Nginx HTTP server was chosen.
Nginx is configured to return empty \gls{http} document which links compiled Elm application.
The actual logic (including routing) is handled in localy in browser.

\todo {Add docker archi figure}
